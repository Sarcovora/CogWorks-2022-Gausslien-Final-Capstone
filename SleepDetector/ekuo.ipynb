{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4b50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3808a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from facenet_models import FacenetModel\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.pyplot import text\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a310244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903bd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eyemodel import EyeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9348d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eyes(image_data, *, box_threshold=0.97, prop_const=0.13):\n",
    "    \"\"\"\n",
    "    Displays an image with boxes around people's faces and labels them with names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_data : numpy.ndarray, shape-(R, C, 3) (RGB is the last dimension)\n",
    "        Pixel information for the image.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    leftBox = []\n",
    "    rightBox = []\n",
    "    \n",
    "    while i < 4:\n",
    "        model = FacenetModel()\n",
    "        \n",
    "        # in the future, only analyze the highest probability face?\n",
    "        boxes, probabilities, landmarks = model.detect(image_data)\n",
    "        face_detected = True\n",
    "\n",
    "        if boxes is None or probabilities[0]<box_threshold:\n",
    "            i+=1\n",
    "            image_data = cv.rotate(image_data, cv.ROTATE_90_CLOCKWISE)\n",
    "            continue\n",
    "        \n",
    "        box = boxes[0]\n",
    "        prob = probabilities[0]\n",
    "        \n",
    "        lefteye = landmarks[0][0]\n",
    "        righteye = landmarks[0][1]\n",
    "\n",
    "        # boxes in form [left,top,right,bottom]\n",
    "\n",
    "        radius = ((box[3] - box[1]) + (box[2] - box[0]))/2 * prop_const\n",
    "\n",
    "        leftBox = np.array([lefteye[0] - radius, \n",
    "                            lefteye[1] - radius, \n",
    "                            lefteye[0] + radius,\n",
    "                            lefteye[1] + radius])\n",
    "        rightBox = np.array([righteye[0] - radius, \n",
    "                            righteye[1] - radius, \n",
    "                            righteye[0] + radius,\n",
    "                            righteye[1] + radius])\n",
    "\n",
    "        leftBox = np.round(np.array(leftBox)).astype(int)\n",
    "        rightBox = np.round(np.array(rightBox)).astype(int)\n",
    "        \n",
    "        break\n",
    "\n",
    "    if len(leftBox)>0:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image_data)\n",
    "\n",
    "        ax.add_patch(Rectangle(leftBox[:2], *(leftBox[2:] - leftBox[:2]), fill=None, lw=2, color=\"yellow\"))\n",
    "        ax.add_patch(Rectangle(rightBox[:2], *(rightBox[2:] - rightBox[:2]), fill=None, lw=2, color=\"yellow\"))\n",
    "        \n",
    "        ax.plot(lefteye[0], lefteye[1], \"+\", color=\"blue\")\n",
    "        ax.plot(righteye[0], righteye[1], \"+\", color=\"blue\")\n",
    "    \n",
    "    return leftBox, rightBox, image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80b70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_img(pic):\n",
    "    if pic.shape[-1] == 4:\n",
    "        # Image is RGBA, where A is alpha -> transparency\n",
    "        # Must make image RGB.\n",
    "        pic = pic[..., :-1]  # png -> RGB\n",
    "\n",
    "    left, right, pic = find_eyes(pic)\n",
    "\n",
    "    if len(left)==0:\n",
    "        print(\"FACE NOT FOUND\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    gray = cv2.cvtColor(pic, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # boxes in form [left,top,right,bottom]\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "\n",
    "    lefteye = gray[left[1]:left[3], left[0]:left[2]]\n",
    "    righteye = gray[right[1]:right[3], right[0]:right[2]]\n",
    "    \n",
    "    lefteye = cv2.resize(lefteye, (24,24), interpolation = cv2.INTER_LINEAR)\n",
    "    righteye = cv2.resize(righteye, (24,24), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    axes[0].imshow(lefteye, cmap=\"gray\")\n",
    "    axes[1].imshow(righteye, cmap=\"gray\")\n",
    "    \n",
    "    return lefteye, righteye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd860a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camera import take_picture\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "\n",
    "def user_interface():\n",
    "    while True:\n",
    "        choice = input(\"Upload (u) or take a photo (c)? \")\n",
    "        if choice==\"u\":\n",
    "            filepath = input(\"Filepath: \")\n",
    "            pic = io.imread(str(filepath))\n",
    "            break\n",
    "        elif choice==\"c\":\n",
    "            pic = take_picture()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Try again. \")\n",
    "        eye_img(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822b4b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''filepath = input(\"Filepath: \")\n",
    "# pic = np.array(Image.open(filepath))[:,:,:3]\n",
    "# shape-(Height, Width, Color)\n",
    "pic = io.imread(str(filepath))\n",
    "if pic.shape[-1] == 4:\n",
    "    # Image is RGBA, where A is alpha -> transparency\n",
    "    # Must make image RGB.\n",
    "    pic = pic[..., :-1]  # png -> RGB\n",
    "i = 0\n",
    "face_detected = False\n",
    "while (not face_detected and i < 4):\n",
    "    leftBox, rightBox, face_detected = find_eyes(pic)\n",
    "    pic = cv.rotate(pic, cv.ROTATE_90_CLOCKWISE)\n",
    "    i += 1\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37edb0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cuda')\n",
    "print(type(cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd239e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EyeModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # assumes 32x32\n",
    "#         self.conv1 = nn.Conv2d(1, 5, 3)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(5, 7, 3)\n",
    "#         self.fc1 = nn.Linear(112, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         #         x = F.softmax(self.fc1(x), dim=1)\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd6ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, numpydata, numpylabels, transform=None, target_transform=None):\n",
    "        self.imgs = numpydata\n",
    "        self.img_labels = numpylabels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.img_labels[idx]\n",
    "        image = self.imgs[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63b94be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#OLD CODE\n",
    "closed_x = np.empty((1,24,24), dtype=int)\n",
    "open_x = np.empty((1,24,24), dtype=int)\n",
    "\n",
    "for filename in glob.glob('train_dataset/closedLeftEyes/*.*'):\n",
    "    img = cv.imread(filename, 0)\n",
    "    closed_x = np.concatenate((closed_x, np.array([img])))\n",
    "    # \tclosed_x.append(img)\n",
    "    \n",
    "for filename in glob.glob('train_dataset/closedRightEyes/*.*'):\n",
    "    img = cv.imread(filename, 0)\n",
    "    closed_x = np.concatenate((closed_x, np.array([img])))\n",
    "    # \tclosed_x.append(img)\n",
    "\n",
    "for filename in glob.glob('train_dataset/openLeftEyes/*.*'):\n",
    "    img = cv.imread(filename, 0)\n",
    "    open_x = np.concatenate((open_x, np.array([img])))\n",
    "    # \topen_x.append(img)\n",
    "\n",
    "for filename in glob.glob('train_dataset/openRightEyes/*.*'):\n",
    "    img = cv.imread(filename, 0)\n",
    "    open_x = np.concatenate((open_x, np.array([img])))\n",
    "    # \topen_x.append(img)\n",
    "# closed_x = np.array(closed_x)\n",
    "# open_x = np.array(open_x)\n",
    "closed_x = closed_x[1:]\n",
    "open_x = open_x[1:]'''\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d5c9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_x = np.empty((1,24,24))\n",
    "open_x = np.empty((1,24,24))\n",
    "\n",
    "for filename in glob.glob('train_dataset/closed*/*.*'):\n",
    "    img = np.array([cv.imread(filename, 0)])/255 + 1\n",
    "\n",
    "    closed_x = np.concatenate((closed_x, img))\n",
    "\n",
    "for filename in glob.glob('train_dataset/open*/*.*'):\n",
    "    img = np.array([cv.imread(filename, 0)])/255 + 1\n",
    "\n",
    "    open_x = np.concatenate((open_x, img))\n",
    "\n",
    "closed_x = closed_x[1:]\n",
    "open_x = open_x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d861eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''print(type(img[0,0]))\n",
    "print(open_x)'''\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ca35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.vstack((closed_x[:int(len(closed_x)*0.8)], open_x[:int(len(open_x)*0.8)]))\n",
    "valid_x = np.vstack((closed_x[int(len(closed_x)*0.8):], open_x[int(len(open_x)*0.8):]))\n",
    "\n",
    "train_y = [0 for i in range(int(len(closed_x)*0.8))] + [1 for i in range(int(len(open_x)*0.8))]\n",
    "valid_y = [0 for i in range(len(closed_x) - int(len(closed_x)*0.8))] + [1 for i in range(len(open_x) - int(len(open_x)*0.8))]\n",
    "# print(train_y[-10])\n",
    "# print(train_y)\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32, device=cuda)\n",
    "valid_x = torch.tensor(valid_x, dtype=torch.float32, device=cuda)\n",
    "\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32, device=cuda)\n",
    "valid_y = torch.tensor(valid_y, dtype=torch.float32, device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d9e45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(train_y[-10])'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_y[-10])'''\n",
    "# print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c683ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    o = torch.argmax(outputs, dim=1)\n",
    "#     print(o.shape)\n",
    "#     print(labels.shape)\n",
    "    return (sum(o == labels) / len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6997434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    70] loss: 0.682\n",
      "[1,    70] accuracy: 0.5\n",
      "[2,    70] loss: 0.559\n",
      "[2,    70] accuracy: 0.71875\n",
      "[3,    70] loss: 0.437\n",
      "[3,    70] accuracy: 0.78125\n",
      "[4,    70] loss: 0.384\n",
      "[4,    70] accuracy: 0.78125\n",
      "[5,    70] loss: 0.327\n",
      "[5,    70] accuracy: 0.90625\n",
      "[6,    70] loss: 0.312\n",
      "[6,    70] accuracy: 1.0\n",
      "[7,    70] loss: 0.259\n",
      "[7,    70] accuracy: 0.9375\n",
      "[8,    70] loss: 0.245\n",
      "[8,    70] accuracy: 0.84375\n",
      "[9,    70] loss: 0.225\n",
      "[9,    70] accuracy: 0.90625\n",
      "[10,    70] loss: 0.219\n",
      "[10,    70] accuracy: 0.84375\n",
      "[11,    70] loss: 0.205\n",
      "[11,    70] accuracy: 0.96875\n",
      "[12,    70] loss: 0.186\n",
      "[12,    70] accuracy: 0.875\n",
      "[13,    70] loss: 0.178\n",
      "[13,    70] accuracy: 0.9375\n",
      "[14,    70] loss: 0.180\n",
      "[14,    70] accuracy: 0.96875\n",
      "[15,    70] loss: 0.171\n",
      "[15,    70] accuracy: 0.875\n",
      "[16,    70] loss: 0.178\n",
      "[16,    70] accuracy: 0.90625\n",
      "[17,    70] loss: 0.152\n",
      "[17,    70] accuracy: 0.96875\n",
      "[18,    70] loss: 0.148\n",
      "[18,    70] accuracy: 0.9375\n",
      "[19,    70] loss: 0.144\n",
      "[19,    70] accuracy: 0.875\n",
      "[20,    70] loss: 0.147\n",
      "[20,    70] accuracy: 0.96875\n",
      "[21,    70] loss: 0.151\n",
      "[21,    70] accuracy: 0.875\n",
      "[22,    70] loss: 0.138\n",
      "[22,    70] accuracy: 0.9375\n",
      "[23,    70] loss: 0.132\n",
      "[23,    70] accuracy: 0.96875\n",
      "[24,    70] loss: 0.132\n",
      "[24,    70] accuracy: 1.0\n",
      "[25,    70] loss: 0.131\n",
      "[25,    70] accuracy: 0.96875\n",
      "[26,    70] loss: 0.129\n",
      "[26,    70] accuracy: 1.0\n",
      "[27,    70] loss: 0.132\n",
      "[27,    70] accuracy: 1.0\n",
      "[28,    70] loss: 0.118\n",
      "[28,    70] accuracy: 0.96875\n",
      "[29,    70] loss: 0.125\n",
      "[29,    70] accuracy: 0.9375\n",
      "[30,    70] loss: 0.127\n",
      "[30,    70] accuracy: 0.90625\n",
      "[31,    70] loss: 0.110\n",
      "[31,    70] accuracy: 0.96875\n",
      "[32,    70] loss: 0.113\n",
      "[32,    70] accuracy: 0.90625\n",
      "[33,    70] loss: 0.106\n",
      "[33,    70] accuracy: 1.0\n",
      "[34,    70] loss: 0.117\n",
      "[34,    70] accuracy: 0.9375\n",
      "[35,    70] loss: 0.112\n",
      "[35,    70] accuracy: 1.0\n",
      "[36,    70] loss: 0.109\n",
      "[36,    70] accuracy: 0.96875\n",
      "[37,    70] loss: 0.103\n",
      "[37,    70] accuracy: 1.0\n",
      "[38,    70] loss: 0.113\n",
      "[38,    70] accuracy: 1.0\n",
      "[39,    70] loss: 0.112\n",
      "[39,    70] accuracy: 0.9375\n",
      "[40,    70] loss: 0.104\n",
      "[40,    70] accuracy: 0.96875\n",
      "[41,    70] loss: 0.099\n",
      "[41,    70] accuracy: 0.96875\n",
      "[42,    70] loss: 0.105\n",
      "[42,    70] accuracy: 0.875\n",
      "[43,    70] loss: 0.097\n",
      "[43,    70] accuracy: 0.96875\n",
      "[44,    70] loss: 0.095\n",
      "[44,    70] accuracy: 1.0\n",
      "[45,    70] loss: 0.103\n",
      "[45,    70] accuracy: 1.0\n",
      "[46,    70] loss: 0.102\n",
      "[46,    70] accuracy: 1.0\n",
      "[47,    70] loss: 0.102\n",
      "[47,    70] accuracy: 0.96875\n",
      "[48,    70] loss: 0.106\n",
      "[48,    70] accuracy: 0.9375\n",
      "[49,    70] loss: 0.097\n",
      "[49,    70] accuracy: 0.96875\n",
      "[50,    70] loss: 0.093\n",
      "[50,    70] accuracy: 0.9375\n",
      "[51,    70] loss: 0.106\n",
      "[51,    70] accuracy: 0.96875\n",
      "[52,    70] loss: 0.095\n",
      "[52,    70] accuracy: 0.96875\n",
      "[53,    70] loss: 0.095\n",
      "[53,    70] accuracy: 0.96875\n",
      "[54,    70] loss: 0.095\n",
      "[54,    70] accuracy: 1.0\n",
      "[55,    70] loss: 0.088\n",
      "[55,    70] accuracy: 0.9375\n",
      "[56,    70] loss: 0.087\n",
      "[56,    70] accuracy: 1.0\n",
      "[57,    70] loss: 0.108\n",
      "[57,    70] accuracy: 0.96875\n",
      "[58,    70] loss: 0.090\n",
      "[58,    70] accuracy: 1.0\n",
      "[59,    70] loss: 0.084\n",
      "[59,    70] accuracy: 0.9375\n",
      "[60,    70] loss: 0.093\n",
      "[60,    70] accuracy: 1.0\n",
      "[61,    70] loss: 0.088\n",
      "[61,    70] accuracy: 0.96875\n",
      "[62,    70] loss: 0.093\n",
      "[62,    70] accuracy: 0.96875\n",
      "[63,    70] loss: 0.084\n",
      "[63,    70] accuracy: 1.0\n",
      "[64,    70] loss: 0.088\n",
      "[64,    70] accuracy: 1.0\n",
      "[65,    70] loss: 0.083\n",
      "[65,    70] accuracy: 1.0\n",
      "[66,    70] loss: 0.083\n",
      "[66,    70] accuracy: 1.0\n",
      "[67,    70] loss: 0.084\n",
      "[67,    70] accuracy: 0.96875\n",
      "[68,    70] loss: 0.076\n",
      "[68,    70] accuracy: 0.96875\n",
      "[69,    70] loss: 0.084\n",
      "[69,    70] accuracy: 1.0\n",
      "[70,    70] loss: 0.082\n",
      "[70,    70] accuracy: 0.96875\n",
      "[71,    70] loss: 0.078\n",
      "[71,    70] accuracy: 1.0\n",
      "[72,    70] loss: 0.080\n",
      "[72,    70] accuracy: 0.9375\n",
      "[73,    70] loss: 0.083\n",
      "[73,    70] accuracy: 0.9375\n",
      "[74,    70] loss: 0.080\n",
      "[74,    70] accuracy: 1.0\n",
      "[75,    70] loss: 0.080\n",
      "[75,    70] accuracy: 0.96875\n",
      "[76,    70] loss: 0.100\n",
      "[76,    70] accuracy: 1.0\n",
      "[77,    70] loss: 0.077\n",
      "[77,    70] accuracy: 0.9375\n",
      "[78,    70] loss: 0.067\n",
      "[78,    70] accuracy: 0.9375\n",
      "[79,    70] loss: 0.098\n",
      "[79,    70] accuracy: 0.96875\n",
      "[80,    70] loss: 0.068\n",
      "[80,    70] accuracy: 1.0\n",
      "[81,    70] loss: 0.075\n",
      "[81,    70] accuracy: 1.0\n",
      "[82,    70] loss: 0.072\n",
      "[82,    70] accuracy: 0.96875\n",
      "[83,    70] loss: 0.069\n",
      "[83,    70] accuracy: 1.0\n",
      "[84,    70] loss: 0.077\n",
      "[84,    70] accuracy: 0.9375\n",
      "[85,    70] loss: 0.075\n",
      "[85,    70] accuracy: 1.0\n",
      "[86,    70] loss: 0.077\n",
      "[86,    70] accuracy: 1.0\n",
      "[87,    70] loss: 0.078\n",
      "[87,    70] accuracy: 0.9375\n",
      "[88,    70] loss: 0.080\n",
      "[88,    70] accuracy: 0.90625\n",
      "[89,    70] loss: 0.070\n",
      "[89,    70] accuracy: 1.0\n",
      "[90,    70] loss: 0.074\n",
      "[90,    70] accuracy: 0.96875\n",
      "[91,    70] loss: 0.081\n",
      "[91,    70] accuracy: 0.9375\n",
      "[92,    70] loss: 0.071\n",
      "[92,    70] accuracy: 1.0\n",
      "[93,    70] loss: 0.075\n",
      "[93,    70] accuracy: 0.96875\n",
      "[94,    70] loss: 0.079\n",
      "[94,    70] accuracy: 1.0\n",
      "[95,    70] loss: 0.066\n",
      "[95,    70] accuracy: 0.96875\n",
      "[96,    70] loss: 0.070\n",
      "[96,    70] accuracy: 0.9375\n",
      "[97,    70] loss: 0.068\n",
      "[97,    70] accuracy: 1.0\n",
      "[98,    70] loss: 0.065\n",
      "[98,    70] accuracy: 0.875\n",
      "[99,    70] loss: 0.075\n",
      "[99,    70] accuracy: 0.96875\n",
      "[100,    70] loss: 0.066\n",
      "[100,    70] accuracy: 0.96875\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = EyeModel().to(cuda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "data = CustomImageDataset(train_x, train_y)\n",
    "trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.reshape(len(inputs),1,24,24)#.float()\n",
    "        labels = labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 70 == 69:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 70:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] accuracy: {accuracy(outputs, labels)}')\n",
    "    \n",
    "    \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb5bb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a337b959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14131179451942444\n",
      "accuracy: 0.9536082744598389\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"model.pb\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = len(valid_x)\n",
    "\n",
    "valid_data = CustomImageDataset(valid_x, valid_y)\n",
    "valid_trainloader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for i, v_data in enumerate(valid_trainloader, 0):\n",
    "        v_inputs, v_labels = v_data\n",
    "        v_inputs = v_inputs.reshape(len(v_inputs),1,24,24)#.float()\n",
    "        v_labels = v_labels.long()\n",
    "        \n",
    "        # forward\n",
    "        v_outputs = model(v_inputs)\n",
    "        loss = criterion(v_outputs, v_labels)\n",
    "\n",
    "        # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         print(f'loss: {running_loss / 2000:.3f}')\n",
    "        print(f'loss: {loss}')\n",
    "        print(f'accuracy: {accuracy(v_outputs, v_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b8322838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for iteration in range(0, epochs):\n",
    "    indices = np.arange(len(train_x))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # trains the model by improving the loss and accuracy metrics for the training data in batches\n",
    "    for batch_count in range(0, len(train_x)//batch_size):\n",
    "        batch_indices = indices[batch_count * batch_size : (batch_count+1) * batch_size]\n",
    "        inputs = train_x[batch_indices]\n",
    "        labels = train_y[batch_indices]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_count) % 10 == 9:\n",
    "            print(f'TRAINING loss is {loss}')\n",
    "        \n",
    "\n",
    "    # creates a shuffled array of testing indices  \n",
    "    test_indices = np.arange(len(valid_x))\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    for batch_count in range(0, len(valid_x)//batch_size):\n",
    "        test_batch_indices = test_indices[batch_count * batch_size : (batch_count+1) * batch_size]\n",
    "        inputs = test_x[test_batch_indices]\n",
    "        labels = test_y[test_batch_indices]\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        if (batch_count) % 10 == 9:\n",
    "            print(f'TESTING loss is {loss}')\n",
    "'''\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
